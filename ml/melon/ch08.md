# 西瓜书概念
## 第8章 集成学习
- Page171: 多分类器系统(multi-classifier system)
  
    即集成学习。

- Page171: 个体学习器（individual learner）

    集成学习的一般结构是：先产生一组“个体学习器”，再用某种策略将它们结合起来，个体学习器通常由一个现有的学习算法从训练数据产生。

- Page171: 基学习器(base learner)

    集成中只包含同种类型的个体学习器，这样的集成是同质的。同质集成中的个体学习器亦称“基学习器”，相应的学习算法称为“基学习算法”。

- Page171: 基学习算法(base learning algorithm)

    见基学习器。

- Page171: 集成学习(311)(ensemble learning)

    集成学习通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统(multi-classifier system)，基于委员会的学习(committee-based learning)。

- Page171: 弱学习器（weak learner）

    集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能，这对弱学习器尤为明显，基学习器有时也被直接称为弱学习器。

- Page172: AdaBoost

    AdaBoost算法有多种推导方式，比较容易理解的是基于“加性模型”，即基学习器的线性组合

    $$H(x) = \sum_{t=1}^T \alpha_t h_t (x)$$

    来最小化指数损失函数（exponential loss function）

    $$l_{exp}(H|D) = \mathbb{E}_{x~D}[e^{-f(x)H(x)}]$$

- Page172: 多样性(diversity)

    学习器之间具有差异。

- Page172: 投票法(225)(voting)

    少数服从多数。

- Page173: Boosting(page139)

    Boosting是一族可将弱学习器提升为强学习器的算法，这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的样本在后续收到更多关注，然后基于调整后的样本分布来训练下一个基学习器，如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。

- Page173: 加性模型

    见AdaBoost

- Page177: 重采样（re-sampling）

    在每一轮学习中，根据样本分布对训练集重新进行采样，再用重采样而得的样本集对基学习器进行训练。

- Page177: 重赋权（re-weighting）

    在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重，对无法接受带权样本的基学习算法，则可通过重采样法处理，两种做法没有显著的优劣差别。

- Page178: Bagging（Boostrap AGGregatING）

    Bagging是并行式集成学习方法最著名的代表，基于自助采样法，给定包含m个样本的数据集，先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样经过m次随机采样操作，我们得到含m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的从未出现。采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合，这就是Bagging的基本流程。

- Page178: 自助采样法(Boostrap sampling)

    见Bagging。
- Page179: 随机森林（Random Forest，RF）

    是Bagging的一个扩展变体，RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机属性选择。具体来说，传统决策树在选择划分属性时是在当前结点的属性集合（假定有d个属性）中选择一个最优属性，，而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分，这里的参数k控制了随机性的引入程度：若令k=d，则基决策树的构建与传统决策树相同；若令k=1，则是随机选择一个属性用于划分；一般情况下，推荐$$k=log_2d$$。

- Page182: 加权平均(225)(weighted averaging)

    假定集成包含T个基学习器$${h_1,h_2,...h_T}$$，其中$$h_i$$在示例$$x$$上的输出为$$h_i(x)$$，加权平均结合$$h_i$$：
    $$H(x)=\sum_{i=1}^Tw_ih_i(x)$$

    其中$$w_i$$是个体学习器$$h_i$$的权重，通常要求$$w_i\geqq0, sum_{i=1}^T=1$$

- Page182: 简单平均(simple averaging)

    $$H(x)=\frac{1}{T}\sum_{i=1}^Th_i(x).$$

    符号含义见加权平均。

- Page182: 绝对多数投票
- Page183: 加权投票(225)
- Page183: 相对多数投票
- Page184: Stacking
- Page185: 贝叶斯模型平均
- Page185: 分歧(304)
- Page185: 误差-分歧分解
- Page187: 差异性度量
- Page187: 多样性度量
- Page189: 属性子集
- Page189: 随机子空间
- Page189: 稳定基学习器
- Page189: 子空间(227)
- Page191: 混合专家
- Page191: 集成修剪
- Page191: 选择性集成
- Page192: Hoeffding不等式(268)
