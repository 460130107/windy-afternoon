# 西瓜书概念


## 第11章 特征选择与稀疏学习


- Page247: 冗余特征  
  
  在特征选择过程中，有一类特征所包含的信息能从其他特征中推演出来，这类特征成为『冗余特征』。  
例如，考虑立方体对象，若已有特征『底面长』、『底面宽』，则『底面积』是冗余特征，因为它能从『底面长』和『底面宽』得到。  
冗余特征很多时候不起作用，去除它们会减轻学习过程的负担。但有时又会降低学习任务的难度。例如若学习目标是估算立方体的体积，则『底面积』这个冗余特征的存在将使得体积的估算更加容易；确切地说，若某个冗余特征恰好对应了完成学习任务所需的『中心概念』，则该冗余特征是有益的。  

- Page247: 数据预处理

  现实世界中数据大体上都是不完整、不一致的脏数据，无法直接进行数据挖掘，或挖掘结果差强人意。为了提高数据挖掘的质量产生了数据预处理技术。它是指在主要的处理以前对数据进行的一些处理。  
数据预处理有多种方法：数据清理，数据集成，数据变换，数据归约等。在现实机器学习任务中，特征选择也是一个重要的数据预处理过程。 

- Page247: 特征选择 & 相关特征

  对一个学习任务，给定的属性集称为特征。对当前学习任务有用的属性称为『相关特征』，没什么用的属性称为『无关特征』。从给定的特征中选出相关特征子集的过程，称为特征选择。
  
- Page247: 相关特征

  见上
  
- Page248: 子集搜索

  从特征集合中选取包含所有重要信息的特征子集，若没有任何领域知识作为先验，就只能遍历所有可能的子集。而这会遭遇组合爆炸问题导致计算不可行。可行的办法是产生一个『候选子集』，评价出它的好坏，基于评价结果产生下一个候选子集，再评价，……直到无法找到更好的候选子集为止。而产生候选子集的过程就是子集搜索。  
  具体而言，给定特征集合 ${a_1, a_2, ...., a_d}$, 可以将每个特征看作一个候选子集，对这 $d$ 个候选单特征子集进行评价，假定 ${a_2}$ 最优，于是将 ${a_2}$ 作为第一轮的选定集；然后，加入一个特征，构成包含两个特征的候选子集，假定在这 $d-1$ 个候选两特征子集中 ${a_2, a_4}$ 最优，且优于 ${a_2}$，于是将 ${a_2, a_4}$ 作为本轮选定集；直至最优的候选特征子集不如上一轮的选定集，则停止生成候选子集，并将上一轮的选定集作为特征选择结果。  
  逐渐增加相关特征的策略称为『前向』搜索；类似的，逐渐减少特征的策略称为『后向』搜索；前向与后向搜索结合起来的策略称为『双向』搜索。
  
- Page248: 子集评价

  由于子集搜索过程仅考虑了本轮选定集最优，无法解决这样的问题：例如在第三轮假定选择 ${a_5}$ 优于 ${a_6}$，于是选定集为 ${a_2, a_4, a_5}$，然而在第四轮却可能是 ${a_2, a_4, a_6, a_8}$ 比所有的 ${a_2, a_4, a_5, a_i}$ 都更优。  
  通过对每个候选特征子集，基于训练数据集计算其信息增益，以此作为评价准则。这一过程称为子集评价。  
  具体而言，给定数据集 $D$，假定 $D$ 中第 $i$ 类样本（假定样本属于离散型）所占的比例为 $p_i(i = 1,2,...,|y|)$。对属性子集 $A$，假定根据其取值将 $D$ 分成了 $V$ 个子集 ${D^1, D^2, ..., D^V}$，每个子集中的样本在 $A$ 上取值相同，于是属性子集 $A$ 的信息增益为：  
  $$Gain(A) = Ent(D) - \sum_{v=1}^{V} \frac {|D^v|}{|D|} Ent(D^v)$$，  
  其中信息熵定义为：  
  $$Ent(D) = -\sum_{i=1}^{|y|} p_k log_2 p_k$$，  
  信息增益 $Gain(A)$ 越大，意味着特征子集 $A$ 包含的有助于分类的信息越多。  
  更一般的，特征子集 $A$ 实际上确定了对数据集 $D$ 的一个划分，每个划分区域对应着 $A$ 上的一个取值，而样本标记信息 $Y$ 则对应着对 $D$ 的真实划分，通过估算这两个划分的差异，就能对 $A$ 进行评价。与 $Y$ 对应的划分的差异越小，则说明 $A$ 越好。
  
- Page249: 过滤式特征选择
- Page250: 包裹式特征选择
- Page251: 拉斯维加斯方法
- Page251: 蒙特卡洛方法(340,384)
- Page252: LASSO(261)
- Page252: Tikhonov正则化
- Page252: 岭回归
- Page252: 嵌入式特征选择
- Page253: L1正则化
- Page253: L2正则化
- Page253: Lipschitz条件
- Page253: 近端梯度下降(259)
- Page255: 码书学习
- Page255: 稀疏编码
- Page255: 字典学习
- Page257: 压缩感知
- Page259: 局部线性嵌入
- Page259: 协调过滤
- Page260: 核范数
- Page260: 迹范数








