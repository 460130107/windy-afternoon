# 西瓜书概念



## 第16章 强化学习


- Page371: MDP
  
  在概率论和统计学中，马可夫决策过程（英语：Markov Decision Processes，缩写为 MDPs）提供了一个数学架构模型，用于面对部份随机，部份可由决策者控制的状态下，如何进行决策，以俄罗斯数学家安德雷·马尔可夫的名字命名。在经由动态规划与强化学习以解决最佳化问题的研究领域中，马可夫决策过程是一个有用的工具。
  
- Page371: 奖赏
  
  奖励函数定义了强化学习 Agent 的目标，它将环境的状态映射为一个数字（奖励），表现了该状态的内在愿望。Agent 的目标是最大限度地提高长期收益。
  
- Page371: 马尔科夫决策过程
  
  Markov Decision Process，通常用来描述强化学习任务：机器处于环境 $E$ 中，状态空间为 $X$，其中每个状态 $x \in X$ 是机器感知到的环境的描述；机器能采取的动作构成了动作空间 $A$；若某个动作 $a \in A$ 作用在当前状态 $x$ 上，则潜在的转移函数 $P$ 将使得环境从当前状态按某种概率转移到另一个状态；在转移到另一个状态的同时，环境会根据潜在的『奖赏』函数 $R$ 反馈给机器一个奖赏。
  
- Page371: 强化学习
  
  强化学习是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。强化学习任务对应了四元组 $E = \langle \mathit{X,A,P,R} \rangle$，其中 $P: X \times A \times X \to \mathbb{R}$ 指定了状态转移概率，$R: X \times A \times X \to \mathbb{R}$ 指定了奖赏；在有的应用中，奖赏函数可能仅与状态转移有关，即 $R: X \times X \to \mathbb{R}$。
  
- Page371: 再励学习
  
  强化学习，亦称再励学习。
  
- Page372: 策略
  
  在环境中状态的转移、奖赏的返回是不受机器控制的，机器只能通过选择要执行的动作来影响环境，也只能通过观察转移后的状态和返回的奖赏来感知环境。  
  机器要做的是通过在环境中不断地尝试而学得一个『策略』（policy）$\pi$，根据这个策略，在状态 $x$ 下就能得知要执行的动作 $a = \pi(x)$。  
  简单来说，policy 是 Agent 的决策功能，规定了在 Agent 可能遇到的任何情况下应采取的行动。这是 Agent 的核心。  
  策略有两种表示方法：一种是将策略表示为函数 $\pi: X \to A$，确定性策略常用这种表示；另一种是概率表示 $\pi: X \times A \to \mathbb{R}$，随机性策略常用这种表示，$\pi(x,a)$ 为状态 $x$ 下选择动作 $a$ 的概率，这里必须有 $\sum_a \pi(x,a) = 1$。  
  在强化学习任务中，学习的目的就是要找到能使长期累积奖赏最大化的策略。
  
- Page373: K-摇臂赌博机
  
  单步强化学习对应的理论模型，K-摇臂赌博机（K-armed bandit）有 K 个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。
  
- Page374: ϵ-贪心
  
  强化学习面临「探索-利用窘境」，$\epsilon$-贪心法基于一个概率来对探索和利用进行折中：每次尝试时，以 $\epsilon$ 的概率进行探索，即以均匀概率随机选取一个摇臂；以 $1 - \epsilon$ 的概率进行利用，即选择当前平均奖赏最高的摇臂（若有多个，则最随机选择一个）。

- Page374: 探索-利用窘境
  
  若获知每个摇臂的期望奖赏，可采用「仅探索」法：将所有的尝试机会平均分配给每个摇臂（即轮流按下每个摇臂），最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。若执行奖赏最大的动作，则可采用「仅利用」法：按下目前最优的（即到目前为止平均奖赏最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。  
  「探索」（即估计摇臂的优劣）和「利用」（即选择当前最优摇臂）这两者是矛盾的，因为尝试次数（即总投币数）有限，加强了一方则会自然削弱另一方，这就是强化学习所面临的「探索-利用窘境」（Exploration-Exploitation dilemma）。
  
- Page375: Softmax
  
  Softmax 算法基于当前已知的摇臂平均奖赏来对探索和利用进行折中。若个摇臂的平均奖赏想当，则选取个摇臂的概率也相当；若某些摇臂的平均奖赏高于其他摇臂，则它们被选取的概率也明显更高。  
  Softmax 算法中摇臂概率的分配是基于 Boltzmann 分布：  
  $$P(k) = \frac {e^{\frac {Q(k)}{\tau}}}{\sum_{i=1}^K e^{\frac {Q(i)}{\tau}}}$$，  
  其中，$Q(i)$ 记录当前摇臂的平均奖赏；$\tau > 0$ 称为「温度」，$\tau$ 越小则平均奖赏高的摇臂被选取的概率越高。$\tau$ 趋于 0 时 Softmax 将趋于「仅利用」，$\tau$ 趋于无穷大时 Softmax 则将趋于「仅探索」。
  
- Page377: 有模型学习
- Page377: 状态-动作值函数
- Page377: 状态值函数
- Page380: Bellman 等式
- Page381: 策略迭代
- Page382: 免模型学习
- Page382: 值迭代
- Page386: TD 学习(393)
- Page386: 时序差分学习(393)
- Page387: Q-学习(393)
- Page387: Sarsa 算法(390)
- Page388: 表格值函数
- Page388: 值函数近似
- Page390: 模仿学习
- Page391: 逆强化学习
- Page393: 近似动态规划
